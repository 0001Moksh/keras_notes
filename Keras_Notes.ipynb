{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68b0ff7c-f29f-4bfa-a16a-b624b9077385",
   "metadata": {},
   "source": [
    "# Keras Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc8a538b-0542-4e3c-b9d4-b744f1e1ebd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\renuk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\renuk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\renuk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\renuk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\renuk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\renuk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\renuk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\renuk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\renuk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\renuk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\renuk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"C:\\Users\\renuk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\renuk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\renuk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\renuk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\renuk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\renuk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\renuk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\renuk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\renuk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\renuk\\AppData\\Local\\Temp\\ipykernel_12388\\3793406994.py\", line 1, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"C:\\Users\\renuk\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\__init__.py\", line 46, in <module>\n",
      "    from tensorflow._api.v2 import __internal__\n",
      "  File \"C:\\Users\\renuk\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py\", line 8, in <module>\n",
      "    from tensorflow._api.v2.__internal__ import autograph\n",
      "  File \"C:\\Users\\renuk\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\_api\\v2\\__internal__\\autograph\\__init__.py\", line 8, in <module>\n",
      "    from tensorflow.python.autograph.core.ag_ctx import control_status_ctx # line: 34\n",
      "  File \"C:\\Users\\renuk\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\autograph\\core\\ag_ctx.py\", line 21, in <module>\n",
      "    from tensorflow.python.autograph.utils import ag_logging\n",
      "  File \"C:\\Users\\renuk\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\autograph\\utils\\__init__.py\", line 17, in <module>\n",
      "    from tensorflow.python.autograph.utils.context_managers import control_dependency_on_returns\n",
      "  File \"C:\\Users\\renuk\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\autograph\\utils\\context_managers.py\", line 19, in <module>\n",
      "    from tensorflow.python.framework import ops\n",
      "  File \"C:\\Users\\renuk\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 50, in <module>\n",
      "    from tensorflow.python.eager import context\n",
      "  File \"C:\\Users\\renuk\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\context.py\", line 37, in <module>\n",
      "    from tensorflow.python.eager import execute\n",
      "  File \"C:\\Users\\renuk\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 21, in <module>\n",
      "    from tensorflow.python.framework import dtypes\n",
      "  File \"C:\\Users\\renuk\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\framework\\dtypes.py\", line 21, in <module>\n",
      "    import ml_dtypes\n",
      "  File \"C:\\Users\\renuk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ml_dtypes\\__init__.py\", line 32, in <module>\n",
      "    from ml_dtypes._finfo import finfo\n",
      "  File \"C:\\Users\\renuk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ml_dtypes\\_finfo.py\", line 19, in <module>\n",
      "    from ml_dtypes._ml_dtypes_ext import bfloat16\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core._multiarray_umath failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mImportError\u001b[0m: numpy.core._multiarray_umath failed to import"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.umath failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\__init__.py:46\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2 \u001b[38;5;28;01mas\u001b[39;00m _tf2\n\u001b[0;32m     44\u001b[0m _tf2\u001b[38;5;241m.\u001b[39menable()\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__ namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decorator\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\_api\\v2\\__internal__\\autograph\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.autograph namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mag_ctx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_status_ctx \u001b[38;5;66;03m# line: 34\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_convert \u001b[38;5;66;03m# line: 493\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\autograph\\core\\ag_ctx.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ag_logging\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_export\n\u001b[0;32m     25\u001b[0m stacks \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mlocal()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\autograph\\utils\\__init__.py:17\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Utility module that contains APIs usable in the generated code.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext_managers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_dependency_on_returns\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m alias_tensors\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor_list\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dynamic_list_append\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\autograph\\utils\\context_managers.py:19\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Various context managers.\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcontextlib\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_array_ops\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontrol_dependency_on_returns\u001b[39m(return_value):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\framework\\ops.py:50\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tf_session\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m monitoring\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\context.py:37\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tf_session\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cancellation\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m execute\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m executor\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m monitoring\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\execute.py:21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tfe\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_conversion_registry\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_shape\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\framework\\dtypes.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdataclasses\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Type, Sequence, Optional\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mml_dtypes\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types_pb2\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ml_dtypes\\__init__.py:32\u001b[0m\n\u001b[0;32m     16\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbfloat16\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muint4\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     28\u001b[0m ]\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Type\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mml_dtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_finfo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m finfo\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mml_dtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_iinfo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m iinfo\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mml_dtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ml_dtypes_ext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bfloat16\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ml_dtypes\\_finfo.py:19\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Overload of numpy.finfo to handle dtypes defined in ml_dtypes.\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mml_dtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ml_dtypes_ext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bfloat16\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mml_dtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ml_dtypes_ext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m float8_e4m3b11fnuz\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mml_dtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ml_dtypes_ext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m float8_e4m3fn\n",
      "\u001b[1;31mImportError\u001b[0m: numpy.core.umath failed to import"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b37c06-6dab-4f2a-8d7a-53740778407c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103eafd3-c3b7-4822-be2c-d8e7490a9b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a constant tensor A\n",
    "A= tf.constant([[4,2],[6,1]])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a18c48e-d7ac-4129-adf1-f10127746c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "V=tf.Variable([[9,2],[6,5]])\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8fee6f-3529-43f4-9451-4209b2c633f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "AV=tf.concat(values=[A,V],axis=0)\n",
    "av=tf.concat(values=[A,V],axis=1)\n",
    "print(AV,'\\n\\n',av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e19ee4-1d20-466f-9c86-4ea92a334dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_tflw=tf.zeros(shape=[3,4],dtype=tf.int32)\n",
    "zero_tflw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f671c1f8-5741-48ae-ad7d-f34fe9767bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_tflw=tf.ones(shape=[3,4],dtype=tf.float32)\n",
    "one_tflw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8355474-822b-4dad-bb94-1c9c6106ff1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identity matrix\n",
    "IM=tf.constant([[1,2],[3,4],[5,6]])\n",
    "rows, columns= IM.shape\n",
    "print('Row:',rows,'\\nColumns:' ,columns)\n",
    "\n",
    "IM_indent=tf.eye(num_rows=rows,num_columns=columns, dtype=tf.int32)\n",
    "print('indentity matrix of IM is:\\n',IM_indent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bced53cb-0360-4e24-9d82-e5faab39659e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_generate=tf.random.uniform(shape=[2,4],dtype=tf.float32)\n",
    "random_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5ce047-4a7c-4d8b-bd43-c397288b1b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_tensor=tf.reshape(tensor= random_generate\n",
    "                          ,shape= [4,2])\n",
    "reshape_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d45adc-21ed-477e-923b-88c5f049edc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# typecast a tensor\n",
    "x=tf.constant([[9,2],[6,5]],dtype=tf.float32)\n",
    "print(x)\n",
    "\n",
    "X=tf.cast(x,tf.int32)\n",
    "print('\\n',X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28b31f4-587f-47b4-b440-f174f5153a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=tf.transpose(X)\n",
    "print('transpose of X:\\n',t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad52395-9e3c-461c-ad4e-5d6cb0d16f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "A=tf.constant([[2,3],[4,5]])\n",
    "V=tf.constant([[4],[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6701301-da86-4b6f-9ed5-a610fe19a926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matmul matrix multiplier\n",
    "AV=tf.matmul(A,V)\n",
    "AV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0e4c98-9740-45ea-b97b-db502b7b3570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element wise multiply\n",
    "av=tf.multiply(A,V)\n",
    "av"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704cb0fa-2b95-4f40-afb0-e97d5c2966b1",
   "metadata": {},
   "source": [
    "# Import the necessary libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ef69b1-d6de-413c-9ae2-236a4780f108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime,warnings,scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"patch.force_edgecolor\"]=True\n",
    "plt.style.use('fivethirtyeight')\n",
    "mpl.rc('patch',edgecolor='dimgray', linewidth=1)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"last_expr\"\n",
    "pd.options.display.max_columns= 50\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53798cfe-e229-48a0-9951-f474cac5ac34",
   "metadata": {},
   "source": [
    "# Import the functions from Keras and sklearn libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a8566b-8baf-498f-9ff3-b09fc9b7c438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610d533a-eb10-4f3a-895f-82af6efac0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('clean_cells.csv')\n",
    "df2=df.groupby(['Date']).mean()\n",
    "df2.describe()\n",
    "# df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4672382c-4db5-438e-90ea-d130f1a03d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55564e5-57a5-45db-9d6e-7b0224131828",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Notes by keras application and me  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463c6d1e-16b4-4bd5-8ff3-9c275b3a4646",
   "metadata": {},
   "source": [
    "You will also need to install a backend framework – either JAX, TensorFlow, or PyTorch:\n",
    "\n",
    "And \n",
    "\n",
    "pip install --upgrade keras-cv\n",
    "\n",
    "pip install --upgrade keras-nlp\n",
    "\n",
    "pip install --upgrade keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc03c74-51ee-4503-9756-60150989dda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"]= \"jax\"\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a1ff27-8cfd-4bd9-a059-f8cdf82a09ac",
   "metadata": {},
   "source": [
    "Note: The backend must be configured before importing Keras, and the backend cannot be changed after the package has been imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3e843a-a432-4975-908c-35b91b32a4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
    "import tensorflow "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c60267-f9bd-4499-90bd-c38933d186ab",
   "metadata": {},
   "source": [
    "Note: These line wouldneed to be before any import tensorfow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd5ab0e-41cc-4198-9dd3-e461a755ba88",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Introduction \n",
    "Keras 3 is a deep learning framework works with TensorFlow, JAX, and PyTorch interchangeably. This notebook will walk you through key Keras 3 workflows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "30267e52-992f-4e8e-886a-366f9b3c40e1",
   "metadata": {},
   "source": [
    "Keras, a popular deep learning library in Python, is divided into several key modules:\n",
    "\n",
    "Layers API: Provides various types of neural network layers, like Dense, Conv2D, LSTM, etc., which can be used to build the architecture of a neural network.\n",
    "\n",
    "Models API: Includes the Sequential model for building simple linear stacks of layers and the Functional API for creating more complex models.\n",
    "\n",
    "Losses API: Offers various loss functions like mean_squared_error, binary_crossentropy, etc., which are used to measure the difference between the predicted and actual outputs.\n",
    "\n",
    "Optimizers API: Contains different optimization algorithms like SGD, Adam, RMSprop, etc., which are used to adjust the weights of the neural network to minimize the loss function.\n",
    "\n",
    "Metrics API: Provides different metrics like accuracy, precision, recall, etc., to evaluate the performance of a model.\n",
    "\n",
    "Callbacks API: Enables the use of custom actions at different stages of training, such as early stopping, learning rate scheduling, etc.\n",
    "\n",
    "Datasets API: Includes pre-built datasets like MNIST, CIFAR-10, IMDB, etc., for training and evaluating models.\n",
    "\n",
    "Utilities API: Contains utility functions for various tasks like image preprocessing, text tokenization, and saving/loading models."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e68d8d41-d808-4ca3-add3-3df4c067c4f9",
   "metadata": {},
   "source": [
    "\n",
    "# Here are some common Keras syntax examples for different tasks:\n",
    "\n",
    "1. Building a Sequential Model:\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Initialize the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers\n",
    "model.add(Dense(units=64, activation='relu', input_shape=(input_dim,)))\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "2. Building a Model using the Functional API:\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "# Define the input layer\n",
    "inputs = Input(shape=(input_dim,))\n",
    "\n",
    "# Add layers\n",
    "x = Dense(64, activation='relu')(inputs)\n",
    "outputs = Dense(10, activation='softmax')(x)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "3. Training a Model:\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "4. Evaluating a Model:\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(x_test, y_test, batch_size=32)\n",
    "print(f'Test loss: {loss}')\n",
    "print(f'Test accuracy: {accuracy}')\n",
    "5. Making Predictions:\n",
    "# Make predictions\n",
    "predictions = model.predict(x_new)\n",
    "6. Using Callbacks:\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# Train the model with callbacks\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
    "7. Saving and Loading a Model:\n",
    "# Save the model\n",
    "model.save('my_model.h5')\n",
    "\n",
    "# Load the model\n",
    "from keras.models import load_model\n",
    "model = load_model('my_model.h5')\n",
    "These examples cover the basics of defining, training, evaluating, and saving models in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fade3555-33a2-4e81-8b5e-1d0851560b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "\n",
    "# Note that Keras should only be imported after the backend\n",
    "# has been configured. The backend cannot be changed once the\n",
    "# package is imported.\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a5aa0c-5290-411d-a516-20efcac04971",
   "metadata": {},
   "source": [
    "A first example: A MNIST convnet\n",
    "Let's start with the Hello World of ML: training a convnet to classify MNIST digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f805a0ac-5361-4422-b2ec-578763acbde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and split it between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f1976e6a-d86a-48b9-81f5-1d269bfbf596",
   "metadata": {},
   "source": [
    "Certainly! Here’s a step-by-step explanation of what each part of the provided code does:\n",
    "\n",
    "1. Load the Data\n",
    "python\n",
    "Copy code\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "What It Does: This line loads the MNIST dataset from Keras. MNIST is a collection of 28x28 grayscale images of handwritten digits (0-9).\n",
    "Outputs:\n",
    "x_train: The training images (60,000 images).\n",
    "y_train: The labels corresponding to the training images (60,000 labels).\n",
    "x_test: The test images (10,000 images).\n",
    "y_test: The labels corresponding to the test images (10,000 labels).\n",
    "2. Scale Images to the [0, 1] Range\n",
    "python\n",
    "Copy code\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "What It Does: This normalizes the pixel values of the images. Originally, pixel values range from 0 to 255. Dividing by 255 scales these values to the range [0, 1], which is more suitable for neural network training.\n",
    "Result: Each pixel value in the images is now a float between 0 and 1.\n",
    "3. Ensure Images Have Shape (28, 28, 1)\n",
    "python\n",
    "Copy code\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "What It Does: Adds an extra dimension to the image arrays to represent the number of color channels. Since MNIST images are grayscale, this dimension will be 1.\n",
    "Result:\n",
    "The shape of x_train changes from (60,000, 28, 28) to (60,000, 28, 28, 1).\n",
    "The shape of x_test changes from (10,000, 28, 28) to (10,000, 28, 28, 1).\n",
    "4. Print Shapes and Number of Samples\n",
    "python\n",
    "Copy code\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "What It Does: Prints the shapes of the image and label arrays and the number of samples in the training and test sets.\n",
    "Outputs:\n",
    "x_train shape: Shows the shape of the training image array (should be (60000, 28, 28, 1)).\n",
    "y_train shape: Shows the shape of the training labels array (should be (60000,)).\n",
    "x_train.shape[0]: Prints the number of training samples (60,000).\n",
    "x_test.shape[0]: Prints the number of test samples (10,000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57746cb1-0d00-46f2-8288-a388fec519ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Input(shape=input_shape),\n",
    "        keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        keras.layers.Conv2D(128, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        keras.layers.Conv2D(128, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        keras.layers.GlobalAveragePooling2D(),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912bac02-477b-43bd-9ff2-db22930e3d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eea327e3-3697-4001-9328-d913db7c4398",
   "metadata": {},
   "source": [
    "The code snippet you provided defines a Convolutional Neural Network (CNN) using Keras. Here’s a detailed step-by-step explanation of each part:\n",
    "\n",
    "1. Define Model Parameters\n",
    "python\n",
    "Copy code\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "num_classes: Number of output classes for classification. MNIST has 10 classes (digits 0-9).\n",
    "input_shape: Shape of the input images, which are 28x28 pixels with 1 color channel (grayscale).\n",
    "2. Define the Model\n",
    "python\n",
    "Copy code\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Input(shape=input_shape),\n",
    "        keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        keras.layers.Conv2D(128, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        keras.layers.Conv2D(128, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        keras.layers.GlobalAveragePooling2D(),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "Explanation of Each Layer:\n",
    "Input Layer:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "keras.layers.Input(shape=input_shape)\n",
    "Purpose: Specifies the shape of the input data (28x28 pixels with 1 color channel). This is the first layer of the model.\n",
    "First Convolutional Layer:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\")\n",
    "Purpose: Applies 64 convolutional filters of size 3x3 to the input image.\n",
    "Activation Function: ReLU (Rectified Linear Unit) introduces non-linearity.\n",
    "Second Convolutional Layer:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\")\n",
    "Purpose: Applies another set of 64 convolutional filters of size 3x3.\n",
    "Activation Function: ReLU.\n",
    "Max Pooling Layer:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
    "Purpose: Reduces the spatial dimensions of the feature maps by taking the maximum value in each 2x2 block.\n",
    "Third Convolutional Layer:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "keras.layers.Conv2D(128, kernel_size=(3, 3), activation=\"relu\")\n",
    "Purpose: Applies 128 convolutional filters of size 3x3.\n",
    "Activation Function: ReLU.\n",
    "Fourth Convolutional Layer:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "keras.layers.Conv2D(128, kernel_size=(3, 3), activation=\"relu\")\n",
    "Purpose: Applies another set of 128 convolutional filters of size 3x3.\n",
    "Activation Function: ReLU.\n",
    "Global Average Pooling Layer:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "keras.layers.GlobalAveragePooling2D()\n",
    "Purpose: Reduces each feature map to a single value by averaging over all spatial dimensions. This flattens the output from the previous layer while preserving the number of feature maps.\n",
    "Dropout Layer:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "keras.layers.Dropout(0.5)\n",
    "Purpose: Applies dropout regularization with a rate of 0.5, randomly setting 50% of the input units to 0 during training to prevent overfitting.\n",
    "Dense Output Layer:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "keras.layers.Dense(num_classes, activation=\"softmax\")\n",
    "Purpose: A fully connected layer with num_classes (10) units. Each unit represents a class probability.\n",
    "Activation Function: Softmax converts the output into a probability distribution over the 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aae3418-d59a-4e7d-85a2-818151cf6813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the compile() method to specify the optimizer, loss function, \n",
    "# and the metrics to monitor. Note that with the JAX and TensorFlow \n",
    "# backends, XLA compilation is turned on by default.\n",
    "\n",
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    metrics=[\n",
    "        keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392f89b3-3296-4ff5-b804-e5c85347db81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train and evaluate the model.\n",
    "# We'll set aside a validation split of 15% of the data during training\n",
    "# to monitor generalization on unseen data.\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 1\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(filepath=\"Moksh_epoch{epoch}.keras\"),\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2),\n",
    "]\n",
    "\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.15,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "score = model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "844f9604-e58a-4b93-a1e8-512ab476163b",
   "metadata": {},
   "source": [
    "\n",
    "The provided code snippet outlines the process to train and evaluate the CNN model. Here’s a detailed step-by-step explanation of each part:\n",
    "\n",
    "1. Set Training Parameters\n",
    "python\n",
    "Copy code\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "batch_size: The number of samples processed before the model is updated. Here, 128 samples are used per batch.\n",
    "epochs: The number of times the entire training dataset will pass through the model. Here, the model will train for 20 epochs.\n",
    "2. Define Callbacks\n",
    "python\n",
    "Copy code\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(filepath=\"model_at_epoch_{epoch}.keras\"),\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2),\n",
    "]\n",
    "ModelCheckpoint: Saves the model's weights at the end of each epoch. The filepath specifies where to save the model and includes {epoch} to save different files for each epoch.\n",
    "EarlyStopping: Stops training early if the validation loss does not improve for a specified number of epochs (patience=2). This helps prevent overfitting and reduces unnecessary computation.\n",
    "3. Train the Model\n",
    "python\n",
    "Copy code\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.15,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "x_train and y_train: The training data and labels.\n",
    "batch_size: The number of samples processed before updating the model weights.\n",
    "epochs: The total number of epochs to train the model.\n",
    "validation_split: A fraction of the training data (15%) used for validation during training. This helps monitor the model’s performance on unseen data while training.\n",
    "callbacks: List of callbacks to apply during training. Here, it includes saving the model and early stopping.\n",
    "4. Evaluate the Model\n",
    "python\n",
    "Copy code\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "x_test and y_test: The test data and labels.\n",
    "verbose=0: Suppresses detailed output during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c100061-cd6f-4ff9-86cc-f7137b65037a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# During training, we were saving a model at the end of each epoch. \n",
    "# You can also save the model in its latest state like this:\n",
    "model.save(\"final_model.keras\")\n",
    "\n",
    "# And reload it like this:\n",
    "model = keras.saving.load_model(\"final_model.keras\")\n",
    "\n",
    "# Next, you can query predictions of class probabilities with predict():\n",
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422fcb44-3607-44cc-98e4-9dd386a7579b",
   "metadata": {},
   "source": [
    "# ______________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7277c96f-670d-476e-a3b1-dfb4b9759848",
   "metadata": {},
   "source": [
    "# Sequential Model (by using MNIST dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6116de02-5ffa-4b7b-8b51-3d02d6d49f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries \n",
    "import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007aaa20-d2f8-49a9-91cc-ff4ac876d6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Perprocess Data\n",
    "model= load_model('mnist_model.h5')\n",
    "\n",
    "# load dataset\n",
    "(x_train,y_train),(x_test,y_test)=mnist.load_data()\n",
    "\n",
    "# Normalize the image to values between 0 to 1\n",
    "x_train= x_train/255.0\n",
    "x_test=x_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9d07df-fa5a-4667-a1a0-73bd987fef4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model= Sequential([\n",
    "    Flatten(input_shape=(28,28)), # Falttenthe 28x28 image to 1D \n",
    "    Dense(128,activation='relu'), # Fully connected layer with 128 units and ReLU activation \n",
    "    # 128 units in the hidden layer, purpose: the hidden layer with 128 units(neurons)\n",
    "    Dense(10,activation='softmax') # Output layer with 10 units for 10 Classes\n",
    "])\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2258849f-421f-4521-98de-968f9f0bfb28",
   "metadata": {},
   "source": [
    "1. *128 Units in Hidden Layer*:\n",
    "   --> Hidden layer mein 128 units (neurons) hote hain jo input data se patterns seekhne aur capture karne ka kaam karte hain. \n",
    "-->Ye sankhya ek aam standard hai jo model ki complexity aur computational efficiency ke beech balance banaati hai.\n",
    "  \n",
    "     * why we use 128 only*: Neurons ki  number ko prayog aur samasya ki zarooraton ke aadhar par chuna jaata hai. \n",
    "       Ismein aam taur par powers of 2 jaise 64, 128, ya 256 ka upyog kiya jaata hai.\n",
    "       Isse aapki samasya aur data ke aadhar par use kiya jata hai.\n",
    "\n",
    "2. *10 Units in Output Layer*:\n",
    "   --> Output layer mein 10 units isliye hote hain kyunki MNIST dataset mein 10 alag-alag classes hain (0 se 9 tak ke ankon ke liye). Har unit ek class ko darshata hai aur softmax activation function in units ke beech probabilities distribute karta hai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f901ea-3d47-48b9-ac29-b788037d7b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy']\n",
    "             )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ed51fd08-d205-432a-a344-edc3ffb315a7",
   "metadata": {},
   "source": [
    "\"Adam kyun use karein? Adam ek popular optimizer hai jo SGD ke advantages ko combine karta hai. Ye learning rate ko adapt karta hai aur noisy gradients ke liye bhi robust hota hai.\n",
    "\n",
    "Alternatives:\n",
    "- SGD: Simple hai lekin learning rate ko tune karna padta hai aur slow converge hota hai.\n",
    "- RMSProp: Ye learning rate ko recent gradients ke moving average ke basis par adjust karta hai.\n",
    "\n",
    "sparse_categorical_crossentropy kyun use karein? Ye multi-class classification problems ke liye hota hai jahan labels integers hote hain. Ye loss true labels aur predicted probabilities ke beech cross-entropy loss calculate karta hai.\n",
    "\n",
    "Alternatives:\n",
    "- categorical_crossentropy: Jab labels one-hot encoded hote hain.\n",
    "- binary_crossentropy: Binary classification problems ke liye.\n",
    "\n",
    "accuracy kyun use karein? Accuracy correct predictions ka proportion measure karta hai aur classification problems ke liye straightforward metric hai.\n",
    "\n",
    "Alternatives:\n",
    "- precision: True positive predictions ka proportion among all positive predictions.\n",
    "- recall: True positive predictions ka proportion among all actual positives.\n",
    "- F1 Score: Precision aur recall ko combine karta hai.\n",
    "- AUC-ROC: Receiver operating characteristic curve ke under area measure karta hai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b040ea7-d699-4dd9-8d36-8d7ec380c38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=5,\n",
    "          validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c275d61-4a0a-4650-ab05-e1db512b660f",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.saving.save_model(model, 'mnist_model.h5')\n",
    "model= tf.keras.models.load_model('mnist_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b616e75-1ee3-4d8e-96cf-0c5ec8e1c252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the model\n",
    "model= tf.keras.models.load_model('mnist_model.h5')\n",
    "\n",
    "# prepare the new model \n",
    "new_image=np.random.rand(28,28)\n",
    "\n",
    "# replace with actual image data\n",
    "new_image= new_image/255.0\n",
    "\n",
    "# normalize\n",
    "new_image= np.expand_dims(new_image, axis=0)\n",
    "\n",
    "# Reshape to (1,28,28)\n",
    "# Make prediction \n",
    "predictions= model.predict(new_image)\n",
    "prediction_class= np.argmax(predictions)\n",
    "print('prediction class is: ',prediction_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de978c6-011d-4fc2-9bec-eb330953e293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_acc= model.evaluate(x_test, y_test,verbose=2 )\n",
    "print(\"Test accuracy\",test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d77db54-f3c2-4465-9acc-757731a64831",
   "metadata": {},
   "source": [
    "# Keras model 2nd level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6e2def-8092-43e3-b86b-2a8973526599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential \n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d14350-1157-4876-8a02-cc08ce053f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.saving.save_model(model, 'mnist_model__2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8b2596-4a4b-4532-8b16-98ba982a7b92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
